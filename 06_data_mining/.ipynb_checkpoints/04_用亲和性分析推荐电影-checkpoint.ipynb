{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "orange-logic",
   "metadata": {},
   "source": [
    "### 1. 亲和性分析\n",
    "- 欺诈检测\n",
    "- 顾客区分\n",
    "- 软件优化\n",
    "- 产品推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-perception",
   "metadata": {},
   "source": [
    "### 1.1 亲和性分析算法\n",
    "Apriori算法可以说是经典的亲和性分析算法<br>\n",
    "Apriori算法： Apriori算法的一个重要参数就是最小支持度\n",
    "- 第一个阶段，需要为Apriori算法指定一个项集要成为频繁项集所需的最小支持度， 任何小于最小支持度的项集将不再考虑<br>\n",
    "    如果最小支持度值过小，Apriori算法要检测大量的项集，会拖慢的运行速度；<br>\n",
    "    最小支持度值过大的话，则只有很少的频繁项集。<br>\n",
    "- 在第二个阶段，根据置信度选取关联规则。可以设定最小置信度<br>\n",
    "    置信度过低将会导致规则支持度高，正确率低；<br>\n",
    "    置信度过高，导致正确率高，但是返回的规则少。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-brain",
   "metadata": {},
   "source": [
    "### 1.2 电影推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "lasting-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "difficult-fisher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating            Datetime\n",
       "0     196      242       3 1997-12-04 15:55:49\n",
       "1     186      302       3 1998-04-04 19:22:22\n",
       "2      22      377       1 1997-11-07 07:18:36\n",
       "3     244       51       2 1997-11-27 05:02:03\n",
       "4     166      346       1 1998-02-02 05:33:16"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/u.data.zip\", delimiter='\\t', header=None, names=[\"UserID\", \"MovieID\", \"Rating\", \"Datetime\"], compression='zip')\n",
    "data[\"Datetime\"] = pd.to_datetime(data[\"Datetime\"], unit='s')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-thing",
   "metadata": {},
   "source": [
    "### 1.3 Apriori算法的实现\n",
    "##### Apriori算法是亲和性分析的一部分，专门用于查找数据集中的频繁项集，基本流程是从前一步找到的频繁项集中找到新的备选集合，接着检测备选集合的频繁程度是否够高，然后迭代\n",
    "- 1. 把项目放到只包含自己的项集中，生成最初的频繁项集，只使用达到最小支持度的项目\n",
    "- 2. 查找现有频繁项集的超集，发现新的频繁项集，并用其生成新的备选项集\n",
    "- 3. 测试新生成的备选项集的频繁程度，如果不够频繁，则舍弃。如果没有新的频繁项集，就跳到最后一步\n",
    "- 4. 存储新发现的频繁项集，跳到步骤 2\n",
    "- 5. 返回所有的频繁项集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "sufficient-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若评分高于3分则是喜欢\n",
    "data['Favorable'] = data['Rating'] > 3\n",
    "ratings = data[data['UserID'].isin(range(200))]\n",
    "favorable_ratings = ratings[ratings['Favorable']]\n",
    "favorable_reviews_by_users = dict((k, frozenset(v.values)) for k, v in favorable_ratings.groupby([\"UserID\"])['MovieID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "uniform-spain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Favorable</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MovieID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Favorable\n",
       "MovieID           \n",
       "50           100.0\n",
       "100           89.0\n",
       "258           83.0\n",
       "181           79.0\n",
       "174           74.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_favorable_by_movie = ratings[['MovieID', 'Favorable']].groupby('MovieID').sum()\n",
    "num_favorable_by_movie.sort_values('Favorable', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-advantage",
   "metadata": {},
   "source": [
    "##### 实现\n",
    "- 把发现的频繁项集保存到以项集长度为键的字典中，便于根据长度查找，找到最新发现的频繁项集\n",
    "- 确定项集要成为频繁项集所需的最小支持度\n",
    "- Apriori算法，项集数随着可用规则的增加而增长一段时间后开始变少，减少是因为项集达不到最低支持度要求，项集的减少是Apriori算法的优点之一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "solid-cheese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 movies with more than 50 favorable reviews\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = {}\n",
    "min_support = 50\n",
    "\n",
    "# 为每一部电影生成只包含它自己的项集，检测他是否频繁，电影编号使用frozenset\n",
    "frequent_itemsets[1] = dict((frozenset((movie_id, )), row['Favorable']) for movie_id, row in num_favorable_by_movie.iterrows() if row['Favorable'] > min_support)\n",
    "print(\"There are {} movies with more than {} favorable reviews\".format(len(frequent_itemsets[1]), min_support))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "wrapped-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 发现新的频繁项集，检测频繁程度\n",
    "def find_frequent_itemsets(favorable_reviews_by_users, k_1_itemsets, min_support):\n",
    "    counts = defaultdict(int)\n",
    "    # 遍历所有用户和打分数据\n",
    "    for user, reviews in favorable_reviews_by_users.items():\n",
    "        # 遍历前面找出的项集，判断它们是否是当前评分项集的子集\n",
    "        for itemset in k_1_itemsets:\n",
    "            if itemset.issubset(reviews):\n",
    "                # 遍历用户打过分却没有出现在项集中的电影，用它生成超集\n",
    "                for other_reviewed_movie in reviews - itemset:\n",
    "                    current_superset = itemset | frozenset((other_reviewed_movie, ))\n",
    "                    counts[current_superset] += 1\n",
    "    # 检测达到支持度要求的项集\n",
    "    return dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency >= min_support])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dietary-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 93 frequent itemsets of length 2\n",
      "I found 295 frequent itemsets of length 3\n",
      "I found 593 frequent itemsets of length 4\n",
      "I found 785 frequent itemsets of length 5\n",
      "I found 677 frequent itemsets of length 6\n",
      "I found 373 frequent itemsets of length 7\n",
      "I found 126 frequent itemsets of length 8\n",
      "I found 24 frequent itemsets of length 9\n",
      "I found 2 frequent itemsets of length 10\n",
      "Did not find any frequent itemsets of length 11\n"
     ]
    }
   ],
   "source": [
    "# 运行Apriori算法，存储算法运行过程中发现的新项集， k表示即将发现的频繁项集的长度\n",
    "for k in range(2, 20):\n",
    "    cur_frequent_itemsets = find_frequent_itemsets(favorable_reviews_by_users, frequent_itemsets[k - 1], min_support)\n",
    "    if len(cur_frequent_itemsets) == 0:\n",
    "        print(\"Did not find any frequent itemsets of length {}\".format(k))\n",
    "        sys.stdout.flush()\n",
    "        break\n",
    "    else:\n",
    "        print(\"I found {} frequent itemsets of length {}\".format(len(cur_frequent_itemsets), k))\n",
    "        frequent_itemsets[k] = cur_frequent_itemsets\n",
    "# 我们对只有一个元素的项集不感兴趣\n",
    "del frequent_itemsets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "pacific-sleeve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 2968 frequent itemsets\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found a total of {sum(len(itemsets) for itemsets in frequent_itemsets.values())} frequent itemsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-transition",
   "metadata": {},
   "source": [
    "### 4.4 抽取关联规则\n",
    "- Apriori算法结束后，我们得到了一系列频繁项集，频繁项集是一组达到最小支持度的项目，而关联规则则由前提和结论组成\n",
    "- 如果用户喜欢前提中的所有电影，那么他们会喜欢结论中的电影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "conservative-sport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15285 candidate rules\n"
     ]
    }
   ],
   "source": [
    "candidates_rules = []\n",
    "# 遍历项集，为项集生成规则\n",
    "for itemset_length, item_counts in frequent_itemsets.items():\n",
    "    for itemset in item_counts.keys():\n",
    "        # 遍历项集中的每一部电影，把它作为结论，项集中的其他电影作为前提，组成备选规则\n",
    "        for conclusion in itemset:\n",
    "            premise = itemset - set((conclusion, ))\n",
    "            candidates_rules.append((premise, conclusion))\n",
    "print(f\"There are {len(candidates_rules)} candidate rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "hispanic-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confidence of each of these rules\n",
    "correct_counts = defaultdict(int) # 正例\n",
    "incorrect_counts = defaultdict(int) # 反例\n",
    "# 遍历所有yoghurt及其喜欢的电影，然后遍历每条规则\n",
    "for user, reviews in favorable_reviews_by_users.items():\n",
    "    for candidate_rule in candidates_rules:\n",
    "        premise, conclusion = candidate_rule\n",
    "        # 判断用户是否喜欢前提中的所有电影\n",
    "        if premise.issubset(reviews):\n",
    "            if conclusion in reviews:\n",
    "                correct_counts[candidate_rule] += 1\n",
    "            else:\n",
    "                incorrect_counts[candidate_rule] += 1\n",
    "# 计算置信度\n",
    "rule_confidence = {candidate_rule: correct_counts[candidate_rule] / float(correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in candidates_rules}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "practical-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule #1: 评论了 frozenset({98, 181}) 的人，他也会评论50\n",
      "- 置信度Confidence: 1.000\n",
      "Rule #2: 评论了 frozenset({172, 79}) 的人，他也会评论174\n",
      "- 置信度Confidence: 1.000\n",
      "Rule #3: 评论了 frozenset({258, 172}) 的人，他也会评论174\n",
      "- 置信度Confidence: 1.000\n",
      "Rule #4: 评论了 frozenset({1, 181, 7}) 的人，他也会评论50\n",
      "- 置信度Confidence: 1.000\n",
      "Rule #5: 评论了 frozenset({1, 172, 7}) 的人，他也会评论174\n",
      "- 置信度Confidence: 1.000\n"
     ]
    }
   ],
   "source": [
    "min_confidence = 0.9\n",
    "rule_confidence = {rule: confidence for rule, confidence in rule_confidence.items() if confidence > min_confidence}\n",
    "\n",
    "sorted_confidence = sorted(rule_confidence.items(), key=lambda x: x[1], reverse=True)\n",
    "for index in range(5):\n",
    "    premise, conclusion = sorted_confidence[index][0]\n",
    "    print(f\"Rule #{index + 1}: 评论了 {premise} 的人，他也会评论{conclusion}\")\n",
    "    print(f'- 置信度Confidence: {rule_confidence[(premise, conclusion)]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "hispanic-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name_data = pd.read_csv('./data/u.item.zip', delimiter='|', header=None, encoding='mac-roman', compression='zip')\n",
    "movie_name_data.columns = [\"MovieID\", \"Title\", \"Release Date\", \"Video Release\", \"IMDB\", \"<UNK>\", \"Action\", \"Adventure\",\n",
    "                           \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\",\n",
    "                           \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dental-roller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get Shorty (1995)'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_movie_name(movie_id, movie_name_data):\n",
    "    title_object = movie_name_data[movie_name_data[\"MovieID\"] == movie_id][\"Title\"]\n",
    "    title = title_object.values[0]\n",
    "    return title\n",
    "get_movie_name(4, movie_name_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "asian-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule #1: 评论了 Silence of the Lambs, The (1991), Return of the Jedi (1983) 的人，他也会评论Star Wars (1977)\n",
      "- 置信度Confidence: 1.000\n",
      "Rule #2: 评论了 Empire Strikes Back, The (1980), Fugitive, The (1993) 的人，他也会评论Raiders of the Lost Ark (1981)\n",
      "- 置信度Confidence: 1.000\n",
      "Rule #3: 评论了 Contact (1997), Empire Strikes Back, The (1980) 的人，他也会评论Raiders of the Lost Ark (1981)\n",
      "- 置信度Confidence: 1.000\n",
      "Rule #4: 评论了 Toy Story (1995), Return of the Jedi (1983), Twelve Monkeys (1995) 的人，他也会评论Star Wars (1977)\n",
      "- 置信度Confidence: 1.000\n",
      "Rule #5: 评论了 Toy Story (1995), Empire Strikes Back, The (1980), Twelve Monkeys (1995) 的人，他也会评论Raiders of the Lost Ark (1981)\n",
      "- 置信度Confidence: 1.000\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    premise, conclusion = sorted_confidence[index][0]\n",
    "    premise_names = \", \".join(get_movie_name(movie_id, movie_name_data) for movie_id in premise)\n",
    "    conclusion_name = get_movie_name(conclusion, movie_name_data)\n",
    "    print(f\"Rule #{index + 1}: 评论了 {premise_names} 的人，他也会评论{conclusion_name}\")\n",
    "    print(f'- 置信度Confidence: {rule_confidence[(premise, conclusion)]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "toxic-therapy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Favorable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>1998-01-07 14:20:06</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>1998-04-03 18:34:27</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>305</td>\n",
       "      <td>451</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-02-01 09:20:17</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>286</td>\n",
       "      <td>1014</td>\n",
       "      <td>5</td>\n",
       "      <td>1997-11-17 15:38:45</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UserID  MovieID  Rating            Datetime  Favorable\n",
       "3      244       51       2 1997-11-27 05:02:03      False\n",
       "5      298      474       4 1998-01-07 14:20:06       True\n",
       "7      253      465       5 1998-04-03 18:34:27       True\n",
       "8      305      451       3 1998-02-01 09:20:17      False\n",
       "11     286     1014       5 1997-11-17 15:38:45       True"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation using test data\n",
    "test_data = data[~data['UserID'].isin(range(200))]\n",
    "test_favorable = test_data[test_data['Favorable']]\n",
    "test_favorable_by_users = dict((k, frozenset(v.values)) for k, v in test_favorable.groupby(['UserID'])['MovieID'])\n",
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "finnish-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts = defaultdict(int)\n",
    "incorrect_counts = defaultdict(int)\n",
    "for user, reviews in test_favorable_by_users.items():\n",
    "    for candidate_rule in candidates_rules:\n",
    "        premise, conclusion = candidate_rule\n",
    "        if premise.issubset(reviews):\n",
    "            if conclusion in reviews:\n",
    "                correct_counts[candidate_rule] += 1\n",
    "            else:\n",
    "                incorrect_counts[candidate_rule] += 1\n",
    "\n",
    "test_confidence = {candidate_rule: correct_counts[candidate_rule] / float(correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in rule_confidence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "manufactured-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_test_confidence = sorted(test_confidence.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "uniform-commodity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule 1: 评论了 Silence of the Lambs, The (1991), Return of the Jedi (1983) 的人, 他也会评论 Star Wars (1977)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.936\n",
      "Rule 2: 评论了 Empire Strikes Back, The (1980), Fugitive, The (1993) 的人, 他也会评论 Raiders of the Lost Ark (1981)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.876\n",
      "Rule 3: 评论了 Contact (1997), Empire Strikes Back, The (1980) 的人, 他也会评论 Raiders of the Lost Ark (1981)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.841\n",
      "Rule 4: 评论了 Toy Story (1995), Return of the Jedi (1983), Twelve Monkeys (1995) 的人, 他也会评论 Star Wars (1977)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.932\n",
      "Rule 5: 评论了 Toy Story (1995), Empire Strikes Back, The (1980), Twelve Monkeys (1995) 的人, 他也会评论 Raiders of the Lost Ark (1981)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.903\n",
      "Rule 6: 评论了 Pulp Fiction (1994), Toy Story (1995), Star Wars (1977) 的人, 他也会评论 Raiders of the Lost Ark (1981)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.816\n",
      "Rule 7: 评论了 Pulp Fiction (1994), Toy Story (1995), Return of the Jedi (1983) 的人, 他也会评论 Star Wars (1977)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.970\n",
      "Rule 8: 评论了 Toy Story (1995), Silence of the Lambs, The (1991), Return of the Jedi (1983) 的人, 他也会评论 Star Wars (1977)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.933\n",
      "Rule 9: 评论了 Toy Story (1995), Empire Strikes Back, The (1980), Return of the Jedi (1983) 的人, 他也会评论 Star Wars (1977)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.971\n",
      "Rule 10: 评论了 Pulp Fiction (1994), Toy Story (1995), Shawshank Redemption, The (1994) 的人, 他也会评论 Silence of the Lambs, The (1991)\n",
      "-训练集上的置信度: 1.000\n",
      "-测试集上的置信度: 0.794\n"
     ]
    }
   ],
   "source": [
    "for index in range(10):\n",
    "    premise, conclusion = sorted_confidence[index][0]\n",
    "    premise_names = ', '.join(get_movie_name(movie_id, movie_name_data) for movie_id in premise)\n",
    "    conclusion_name = get_movie_name(conclusion, movie_name_data)\n",
    "    print(f\"Rule {index + 1}: 评论了 {premise_names} 的人, 他也会评论 {conclusion_name}\")\n",
    "    print(f'-训练集上的置信度: {rule_confidence.get((premise, conclusion), -1):.3f}')\n",
    "    print(f'-测试集上的置信度: {test_confidence.get((premise, conclusion), -1):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-projection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
