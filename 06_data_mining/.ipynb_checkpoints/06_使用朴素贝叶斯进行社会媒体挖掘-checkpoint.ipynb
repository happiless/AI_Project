{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "technological-theater",
   "metadata": {},
   "source": [
    "### åŒ…å«å†…å®¹\n",
    "- ç”¨ç¤¾äº¤ç½‘ç»œçš„APIä¸‹è½½æ•°æ®\n",
    "- ç”¨äºå¤„ç†æ–‡æœ¬çš„è½¬æ¢å™¨\n",
    "- æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨\n",
    "- ç”¨JSONä¿å­˜å’ŒåŠ è½½æ•°æ®é›†\n",
    "- ç”¨NLTKåº“ä»æ–‡æœ¬ä¸­æŠ½å–ç‰¹å¾\n",
    "- ç”¨Få€¼è¯„ä¼°åˆ†ç±»æ•ˆæœ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-liberia",
   "metadata": {},
   "source": [
    "### 1. æ¶ˆæ­§\n",
    "- 1. æ–‡æœ¬é€šå¸¸è¢«ç§°ä¸ºæ— ç»“æ„æ ¼å¼ï¼Œ\n",
    "- 2. æ–‡æœ¬æŒ–æ˜çš„ä¸€ä¸ªéš¾ç‚¹æ¥è‡ªäºæ­§ä¹‰ï¼Œæ¶ˆé™¤æ­§ä¹‰å¸¸è¢«ç®€ç§°ä¸ºæ¶ˆæ­§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "appreciated-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweepy twitter nltk -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hundred-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "documented-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"hEDGEdp4p2Tp8U6h5vpOGOpm9\"\n",
    "consumer_secret = \"4C8P1YeOtsjebf6dYyTnRCWae3GWWoF9n6VLnQOKTEorlK3KbT\"\n",
    "access_token = \"846992744690208768-QQKUada7IjhguoQyjrARQ99nCkHVMop\"\n",
    "access_token_secret = \"5zfmqKV2eLPO2SU0XoW3DKtOUqwmvpPB1r7KoQ48OQnq8\"\n",
    "authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\n",
    "t = twitter.Twitter(auth=authorization)\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)  \n",
    "for status in tweepy.Cursor(api.home_timeline).items(2):\n",
    "    print (status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dangerous-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "with open('./data/python_tweets.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '': continue\n",
    "        tweets.append(json.loads(line)['text'])\n",
    "print(\"Loaded {} tweets\".format(len(tweets)))\n",
    "labels = []\n",
    "with open('./data/python_classes.json', 'r', encoding='utf-8') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bigger-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_tweet():\n",
    "    return tweets[len(labels)]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-sigma",
   "metadata": {},
   "source": [
    "### Classify Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-belly",
   "metadata": {},
   "source": [
    "### æœ´ç´ è´å¶æ–¯\n",
    "- è´å¶æ–¯å®šç†å…¬å¼<br>\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-narrow",
   "metadata": {},
   "source": [
    "- å«æœ‰å•è¯drugsçš„ä¸ºåƒåœ¾é‚®ä»¶çš„æ¦‚ç‡\n",
    "    * P(A) ä¸ºåƒåœ¾é‚®ä»¶, P(A)å°±æ˜¯å…ˆéªŒæ¦‚ç‡\n",
    "    * P(B) è¡¨ç¤ºè¯¥å°é‚®ä»¶å«æœ‰drugs, è®¡ç®—P(B)æ—¶ï¼Œæˆ‘ä»¬ä¸å…³æ³¨é‚®ä»¶æ˜¯ä¸æ˜¯åƒåœ¾é‚®ä»¶\n",
    "    * P(B|A) æŒ‡çš„æ˜¯åƒåœ¾é‚®ä»¶ä¸­å«æœ‰å•è¯drugsçš„æ¦‚ç‡ï¼Œç»Ÿè®¡æ‰€æœ‰åƒåœ¾é‚®ä»¶çš„æ•°é‡ä»¥åŠå«æœ‰å•è¯drugsçš„æ•°é‡\n",
    "    * P(A|B) = P(B|A)P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-athens",
   "metadata": {},
   "source": [
    "- ç”¨Cè¡¨ç¤ºæŸç§ç±»åˆ«, ç”¨ D è¡¨ç¤ºæ•°æ®é›†ä¸­ä¸€ç¯‡æ–‡æ¡£\n",
    "    * P(C) ä¸ºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡\n",
    "    * P(D) ä¸ºæŸä¸€æ–‡æ¡£çš„æ¦‚ç‡ï¼Œç‰µæ‰¯åˆ°å¾ˆå¤šç‰¹å¾ï¼Œè®¡ç®—æ¯”è¾ƒå›°éš¾ï¼Œå¯¹äºæ‰€æœ‰ç±»åˆ«æ¥è¯´ P(D) ç›¸åŒ\n",
    "    * P(D|C) ä¸ºæ–‡æ¡£ D å±äº C ç±»çš„æ¦‚ç‡ï¼Œ æœ´ç´ è´å¶æ–¯å‡è®¾å„ä¸ªç‰¹å¾ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œåˆ†åˆ«æ±‚D1ï¼ŒD2ï¼ŒD3çš„æ¦‚ç‡ï¼Œå†æ±‚ç§¯<br>\n",
    "    P(D|C) = P(D1|C) * P(D2|C) *...*P(Dn|C)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-storage",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹\n",
    "- å‡å¦‚æ•°æ®é›†ä¸­æœ‰ä»¥ä¸‹ä¸€æ¡ç”¨äºŒå€¼ç‰¹å¾è¡¨ç¤ºçš„æ•°æ®ï¼š[1, 0, 0, 1]ã€‚è®­ç»ƒé›†ä¸­æœ‰75%çš„æ•°æ®å±äºç±»åˆ«0, 25%å±äºç±»åˆ«1ï¼Œä¸”æ¯ä¸ªç‰¹å¾çš„å±äºæ¯ä¸ªç±»åˆ«çš„ä¼¼ç„¶åº¦å¦‚ä¸‹ï¼š\n",
    "    * ç±»åˆ«0ï¼š[0.3, 0.4, 0.4, 0.7]\n",
    "    * ç±»åˆ«1ï¼š[0.7, 0.3, 0.4, 0.9]\n",
    "    * ä¸Šé¢çš„æ•°æ®å¯ä»¥ç†è§£ä¸ºï¼šç±»åˆ«0ä¸­æœ‰30%çš„æ•°æ®ï¼Œç‰¹å¾å€¼ä¸º1\n",
    "- P(C=0) = 0.75\n",
    "    * å› ä¸ºç”¨ä¸åˆ°P(D), æ‰€ä»¥ä¸éœ€è¦è®¡ç®—\n",
    "    * P(D|C=0) = P(D1|C=0) * P(D2|C=0) * P(D3|C=0) * P(D4|C=0)\n",
    "    * = 0.3 * 0.6 * 0.6 * 0.7\n",
    "    * = 0.0756\n",
    "- P(C=0|D) = P(C=0) * P(D|C=0) = 0.75 * 0.0756 = 0.0567\n",
    "- P(C=1) = 0.25\n",
    "    * P(D|C=1) = P(D1|C=1) x P(D2|C=1) x P(D3|C=1) x P(D4|C=1)\n",
    "    * = 0.7 x 0.7 x 0.6 x 0.9 \n",
    "    * = 0.2646 \n",
    "- P(C=1|D) = P(C=1)P(D|C=1) = 0.25 * 0.2646 = 0.06615 \n",
    "- æ³¨æ„: é€šå¸¸ï¼ŒP(C=0|D) + P(C=1|D)åº”è¯¥ç­‰äº1ï¼Œä½†æ˜¯å› ä¸ºæˆ‘ä»¬è®¡ç®—æ—¶çœå»äº† P(D), æ‰€ä»¥æœ´ç´ è´å¶æ–¯çš„æ¦‚ç‡å’Œå¹¶ä¸ä¸º1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "enclosed-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "awful-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = min(len(tweets), len(labels))\n",
    "sample_tweets = [t.lower() for t in tweets[:n_samples]]\n",
    "labels = labels[:n_samples]\n",
    "y_true = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "drawn-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # å­—å…¸ä¸­çš„å„é¡¹ä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯çš„æ‰€æœ‰è¯è¯­ï¼Œæ¯ä¸€é¡¹ç”¨å•è¯ä½œä¸ºé”®ï¼Œå€¼ä¸ºTrueï¼Œè¡¨ç¤ºè¯¥è¯åœ¨è¯¥æ¡æ¶ˆæ¯ä¸­å‡ºç°è¿‡ï¼Œå­—å…¸ä¸­æ²¡å‡ºç°çš„è¯ï¼Œè¡¨ç¤ºè¿™æ¡æ¶ˆæ¯é‡Œä¸åŒ…å«è¯¥è¯\n",
    "        # ä¹Ÿå¯ä»¥ä½¿ç”¨Falseä»£è¡¨ä¸å­˜åœ¨çš„è¯ï¼Œä¸è¿‡å¤ªæµªè´¹å­˜å‚¨ç©ºé—´äº†\n",
    "        return [{word: True for word in word_tokenize(document)} for document in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "electronic-portal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "pipeline = Pipeline([('bag-of-words', NLTKBOW()), ('vectorizer', DictVectorizer()), ('naive-bayes', BernoulliNB())])\n",
    "scores = cross_val_score(pipeline, sample_tweets, y_true, cv=10, scoring='f1')\n",
    "print(\"Score: {:.3f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "pursuant-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "bow = NLTKBOW()\n",
    "vectorizer = DictVectorizer()\n",
    "clf = BernoulliNB()\n",
    "X_bow = bow.fit_transform(sample_tweets)\n",
    "X_vec = vectorizer.fit_transform(X_bow)\n",
    "scores = cross_val_score(pipeline, sample_tweets, y_true, cv=10, scoring='f1')\n",
    "print(\"Score: {:.3f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-pacific",
   "metadata": {},
   "source": [
    "##### ç”¨F1å€¼è¯„ä¼°\n",
    "- æ­£ç¡®ç‡åº”ç”¨èŒƒå›´å¾ˆå¹¿ï¼Œç†è§£èµ·æ¥æ¯”è¾ƒå®¹æ˜“ï¼Œè®¡ç®—èµ·æ¥ä¹Ÿæ–¹ä¾¿ï¼Œä½†æ˜¯åœ¨æ ·æœ¬é›†ä¸å‡åŒ€çš„æ—¶å€™æ•ˆæœä¸æ˜¯å¾ˆå¥½\n",
    "- F1å€¼æ˜¯ä»¥æ¯ä¸ªç±»åˆ«ä¸ºåŸºç¡€è¿›è¡Œå®šä¹‰çš„ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡(precision) å’Œ å¬å›ç‡(recall)\n",
    "    * å‡†ç¡®ç‡æ˜¯æŒ‡é¢„æµ‹ç»“æœå±äºæŸä¸€ç±»çš„ä¸ªä½“ï¼Œå®é™…å±äºè¯¥ç±»çš„æ¯”ä¾‹\n",
    "    * å¬å›ç‡æ˜¯æŒ‡è¢«æ­£ç¡®é¢„æµ‹ä¸ºæŸä¸ªç±»åˆ«çš„ä¸ªä½“æ•°é‡ä¸æ•°æ®é›†ä¸­è¯¥ç±»åˆ«ä¸ªä½“æ€»é‡çš„æ¯”ä¾‹\n",
    "- F1å€¼ä¸ºå‡†ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°<br>\n",
    "$$ F1 = 2 * \\frac{precision * recall}{precision + recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-selection",
   "metadata": {},
   "source": [
    "##### ä»æ¨¡å‹ä¸­è·å–æ›´å¤šæœ‰ç”¨çš„ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "congressional-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : -0.2231435513142097\n",
      "1 @ -0.5108256237659905\n",
      "2 https -0.5108256237659905\n",
      "3 rt -0.5108256237659905\n",
      "4 ! -0.916290731874155\n",
      "5 ğŸ“° -0.916290731874155\n",
      "6 initiated -0.916290731874155\n",
      "7 httpsâ€¦ -0.916290731874155\n",
      "8 getting -0.916290731874155\n",
      "9 from -0.916290731874155\n",
      "10 docprogrammer -0.916290731874155\n",
      "11 called -0.916290731874155\n",
      "12 murderous -0.916290731874155\n",
      "13 by -0.916290731874155\n",
      "14 away -0.916290731874155\n",
      "15 army -0.916290731874155\n",
      "16 and -0.916290731874155\n",
      "17 alert -0.916290731874155\n",
      "18 ] -0.916290731874155\n",
      "19 [ -0.916290731874155\n",
      "20 docker -0.916290731874155\n",
      "21 news -0.916290731874155\n",
      "22 nginx -0.916290731874155\n",
      "23 ngiâ€¦ -0.916290731874155\n",
      "24 ğŸƒ -0.916290731874155\n",
      "25 ğŸ‡³ğŸ‡¬ -0.916290731874155\n",
      "26 æ–°ç€ -0.916290731874155\n",
      "27 â˜ -0.916290731874155\n",
      "28 â€” -0.916290731874155\n",
      "29 with -0.916290731874155\n",
      "30 vaccination -0.916290731874155\n",
      "31 tutorial -0.916290731874155\n",
      "32 stay -0.916290731874155\n",
      "33 started -0.916290731874155\n",
      "34 sam_ezeh -0.916290731874155\n",
      "35 redis -0.916290731874155\n",
      "36 pythonã«ã‚ˆã‚‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åˆå¿ƒè€…ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿è‡ªå‹•åé›† -0.916290731874155\n",
      "37 python -0.916290731874155\n",
      "38 programme -0.916290731874155\n",
      "39 poisonous -0.916290731874155\n",
      "40 opâ€¦ -0.916290731874155\n",
      "41 on-going -0.916290731874155\n",
      "42 nigerian -0.916290731874155\n",
      "43 2017/10/28ï¼ˆåœŸï¼‰ -0.916290731874155\n",
      "44 //t.co/mqh3fmwxbu -0.916290731874155\n",
      "45 ğŸš¨ -0.916290731874155\n",
      "46 //t.co/angwwlf6ut -0.916290731874155\n",
      "47 # -0.916290731874155\n",
      "48 , -0.916290731874155\n",
      "49 . -0.916290731874155\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(sample_tweets, labels)\n",
    "nb = model.named_steps['naive-bayes']\n",
    "top_features = np.argsort(-nb.feature_log_prob_[1])[:50]\n",
    "\n",
    "# é€šè¿‡DictVectorizerçš„feature_names_å±æ€§æŸ¥æ‰¾æœ€ä½³ç‰¹å¾åç§°\n",
    "dv = model.named_steps['vectorizer']\n",
    "for i, feature_index in enumerate(top_features):\n",
    "    print(i, dv.feature_names_[feature_index], nb.feature_log_prob_[1][feature_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "color-track",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/python_context.pkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "output_filename = os.path.join('./data/', \"python_context.pkl\")\n",
    "joblib.dump(model, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
