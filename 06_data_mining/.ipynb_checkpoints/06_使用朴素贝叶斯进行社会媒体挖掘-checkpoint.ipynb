{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "technological-theater",
   "metadata": {},
   "source": [
    "### 包含内容\n",
    "- 用社交网络的API下载数据\n",
    "- 用于处理文本的转换器\n",
    "- 朴素贝叶斯分类器\n",
    "- 用JSON保存和加载数据集\n",
    "- 用NLTK库从文本中抽取特征\n",
    "- 用F值评估分类效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-liberia",
   "metadata": {},
   "source": [
    "### 1. 消歧\n",
    "- 1. 文本通常被称为无结构格式，\n",
    "- 2. 文本挖掘的一个难点来自于歧义，消除歧义常被简称为消歧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "appreciated-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweepy twitter nltk -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hundred-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "documented-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"hEDGEdp4p2Tp8U6h5vpOGOpm9\"\n",
    "consumer_secret = \"4C8P1YeOtsjebf6dYyTnRCWae3GWWoF9n6VLnQOKTEorlK3KbT\"\n",
    "access_token = \"846992744690208768-QQKUada7IjhguoQyjrARQ99nCkHVMop\"\n",
    "access_token_secret = \"5zfmqKV2eLPO2SU0XoW3DKtOUqwmvpPB1r7KoQ48OQnq8\"\n",
    "authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\n",
    "t = twitter.Twitter(auth=authorization)\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)  \n",
    "for status in tweepy.Cursor(api.home_timeline).items(2):\n",
    "    print (status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dangerous-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "with open('./data/python_tweets.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '': continue\n",
    "        tweets.append(json.loads(line)['text'])\n",
    "print(\"Loaded {} tweets\".format(len(tweets)))\n",
    "labels = []\n",
    "with open('./data/python_classes.json', 'r', encoding='utf-8') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bigger-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_tweet():\n",
    "    return tweets[len(labels)]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-sigma",
   "metadata": {},
   "source": [
    "### Classify Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-belly",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯\n",
    "- 贝叶斯定理公式<br>\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-narrow",
   "metadata": {},
   "source": [
    "- 含有单词drugs的为垃圾邮件的概率\n",
    "    * P(A) 为垃圾邮件, P(A)就是先验概率\n",
    "    * P(B) 表示该封邮件含有drugs, 计算P(B)时，我们不关注邮件是不是垃圾邮件\n",
    "    * P(B|A) 指的是垃圾邮件中含有单词drugs的概率，统计所有垃圾邮件的数量以及含有单词drugs的数量\n",
    "    * P(A|B) = P(B|A)P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-athens",
   "metadata": {},
   "source": [
    "- 用C表示某种类别, 用 D 表示数据集中一篇文档\n",
    "    * P(C) 为某一类别的概率\n",
    "    * P(D) 为某一文档的概率，牵扯到很多特征，计算比较困难，对于所有类别来说 P(D) 相同\n",
    "    * P(D|C) 为文档 D 属于 C 类的概率， 朴素贝叶斯假设各个特征之间是相互独立的，分别求D1，D2，D3的概率，再求积<br>\n",
    "    P(D|C) = P(D1|C) * P(D2|C) *...*P(Dn|C)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-storage",
   "metadata": {},
   "source": [
    "##### 示例\n",
    "- 假如数据集中有以下一条用二值特征表示的数据：[1, 0, 0, 1]。训练集中有75%的数据属于类别0, 25%属于类别1，且每个特征的属于每个类别的似然度如下：\n",
    "    * 类别0：[0.3, 0.4, 0.4, 0.7]\n",
    "    * 类别1：[0.7, 0.3, 0.4, 0.9]\n",
    "    * 上面的数据可以理解为：类别0中有30%的数据，特征值为1\n",
    "- P(C=0) = 0.75\n",
    "    * 因为用不到P(D), 所以不需要计算\n",
    "    * P(D|C=0) = P(D1|C=0) * P(D2|C=0) * P(D3|C=0) * P(D4|C=0)\n",
    "    * = 0.3 * 0.6 * 0.6 * 0.7\n",
    "    * = 0.0756\n",
    "- P(C=0|D) = P(C=0) * P(D|C=0) = 0.75 * 0.0756 = 0.0567\n",
    "- P(C=1) = 0.25\n",
    "    * P(D|C=1) = P(D1|C=1) x P(D2|C=1) x P(D3|C=1) x P(D4|C=1)\n",
    "    * = 0.7 x 0.7 x 0.6 x 0.9 \n",
    "    * = 0.2646 \n",
    "- P(C=1|D) = P(C=1)P(D|C=1) = 0.25 * 0.2646 = 0.06615 \n",
    "- 注意: 通常，P(C=0|D) + P(C=1|D)应该等于1，但是因为我们计算时省去了 P(D), 所以朴素贝叶斯的概率和并不为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "enclosed-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "awful-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = min(len(tweets), len(labels))\n",
    "sample_tweets = [t.lower() for t in tweets[:n_samples]]\n",
    "labels = labels[:n_samples]\n",
    "y_true = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "drawn-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # 字典中的各项为第一条消息的所有词语，每一项用单词作为键，值为True，表示该词在该条消息中出现过，字典中没出现的词，表示这条消息里不包含该词\n",
    "        # 也可以使用False代表不存在的词，不过太浪费存储空间了\n",
    "        return [{word: True for word in word_tokenize(document)} for document in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "electronic-portal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "pipeline = Pipeline([('bag-of-words', NLTKBOW()), ('vectorizer', DictVectorizer()), ('naive-bayes', BernoulliNB())])\n",
    "scores = cross_val_score(pipeline, sample_tweets, y_true, cv=10, scoring='f1')\n",
    "print(\"Score: {:.3f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "pursuant-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "bow = NLTKBOW()\n",
    "vectorizer = DictVectorizer()\n",
    "clf = BernoulliNB()\n",
    "X_bow = bow.fit_transform(sample_tweets)\n",
    "X_vec = vectorizer.fit_transform(X_bow)\n",
    "scores = cross_val_score(pipeline, sample_tweets, y_true, cv=10, scoring='f1')\n",
    "print(\"Score: {:.3f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-pacific",
   "metadata": {},
   "source": [
    "##### 用F1值评估\n",
    "- 正确率应用范围很广，理解起来比较容易，计算起来也方便，但是在样本集不均匀的时候效果不是很好\n",
    "- F1值是以每个类别为基础进行定义的，包括准确率(precision) 和 召回率(recall)\n",
    "    * 准确率是指预测结果属于某一类的个体，实际属于该类的比例\n",
    "    * 召回率是指被正确预测为某个类别的个体数量与数据集中该类别个体总量的比例\n",
    "- F1值为准确率和召回率的调和平均数<br>\n",
    "$$ F1 = 2 * \\frac{precision * recall}{precision + recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-selection",
   "metadata": {},
   "source": [
    "##### 从模型中获取更多有用的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "congressional-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : -0.2231435513142097\n",
      "1 @ -0.5108256237659905\n",
      "2 https -0.5108256237659905\n",
      "3 rt -0.5108256237659905\n",
      "4 ! -0.916290731874155\n",
      "5 📰 -0.916290731874155\n",
      "6 initiated -0.916290731874155\n",
      "7 https… -0.916290731874155\n",
      "8 getting -0.916290731874155\n",
      "9 from -0.916290731874155\n",
      "10 docprogrammer -0.916290731874155\n",
      "11 called -0.916290731874155\n",
      "12 murderous -0.916290731874155\n",
      "13 by -0.916290731874155\n",
      "14 away -0.916290731874155\n",
      "15 army -0.916290731874155\n",
      "16 and -0.916290731874155\n",
      "17 alert -0.916290731874155\n",
      "18 ] -0.916290731874155\n",
      "19 [ -0.916290731874155\n",
      "20 docker -0.916290731874155\n",
      "21 news -0.916290731874155\n",
      "22 nginx -0.916290731874155\n",
      "23 ngi… -0.916290731874155\n",
      "24 🏃 -0.916290731874155\n",
      "25 🇳🇬 -0.916290731874155\n",
      "26 新着 -0.916290731874155\n",
      "27 ☞ -0.916290731874155\n",
      "28 — -0.916290731874155\n",
      "29 with -0.916290731874155\n",
      "30 vaccination -0.916290731874155\n",
      "31 tutorial -0.916290731874155\n",
      "32 stay -0.916290731874155\n",
      "33 started -0.916290731874155\n",
      "34 sam_ezeh -0.916290731874155\n",
      "35 redis -0.916290731874155\n",
      "36 pythonによるスクレイピング初心者のためのデータ自動収集 -0.916290731874155\n",
      "37 python -0.916290731874155\n",
      "38 programme -0.916290731874155\n",
      "39 poisonous -0.916290731874155\n",
      "40 op… -0.916290731874155\n",
      "41 on-going -0.916290731874155\n",
      "42 nigerian -0.916290731874155\n",
      "43 2017/10/28（土） -0.916290731874155\n",
      "44 //t.co/mqh3fmwxbu -0.916290731874155\n",
      "45 🚨 -0.916290731874155\n",
      "46 //t.co/angwwlf6ut -0.916290731874155\n",
      "47 # -0.916290731874155\n",
      "48 , -0.916290731874155\n",
      "49 . -0.916290731874155\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(sample_tweets, labels)\n",
    "nb = model.named_steps['naive-bayes']\n",
    "top_features = np.argsort(-nb.feature_log_prob_[1])[:50]\n",
    "\n",
    "# 通过DictVectorizer的feature_names_属性查找最佳特征名称\n",
    "dv = model.named_steps['vectorizer']\n",
    "for i, feature_index in enumerate(top_features):\n",
    "    print(i, dv.feature_names_[feature_index], nb.feature_log_prob_[1][feature_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "color-track",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/python_context.pkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "output_filename = os.path.join('./data/', \"python_context.pkl\")\n",
    "joblib.dump(model, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
